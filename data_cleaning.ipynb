{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from math import ceil\n",
    "from textblob import TextBlob\n",
    "# import cPickle\n",
    "\n",
    "def clean_words(words, verbose=True):\n",
    "    # returns cleans words so they are usable\n",
    "    junk = \",.:?!\"                              # Punctuation I dont want\n",
    "    poss_is = \"'s\"                              # remove possessive since doesnt add syllable and is not in corpus\n",
    "    poss_are = \"'re\"\n",
    "    new_words = []                             # list to collect all the cleaned words\n",
    "    for word in words:\n",
    "        word = str(word)\n",
    "        word = str(word.strip())\n",
    "        if poss_is in word and poss_is == word[-2:]:\n",
    "            word = word.replace(poss_is,'')     # removed poss_is\n",
    "        elif poss_are in word and poss_are == word[-3:]:\n",
    "            word = word.replace(poss_are, '')   # remove poss_are\n",
    "        elif word[-2:] == \"'d\":\n",
    "            word = word.replace(\"'d\",\"\")\n",
    "        else:\n",
    "            word = word.replace(\"'\",\"\")\n",
    "        if '...' in word:\n",
    "            word = word.replace('...','')\n",
    "        for j in junk:\n",
    "            word = word.replace(j,'')           # remove junk\n",
    "        if '--' in word:\n",
    "            word = word.replace('-','')\n",
    "        elif '-' in word:                         # splits hyphenated words\n",
    "            hyph = word.split('-')\n",
    "            new_words.append(hyph[0].strip())\n",
    "            new_words.append(hyph[1].strip())\n",
    "            # new_words.append(hyph[0].encode('ascii', 'ignore').strip())\n",
    "            # new_words.append(hyph[1].encode('ascii', 'ignore').strip())\n",
    "        else:\n",
    "            new_words.append(word)              # put cleaned word in new_words\n",
    "    return new_words\n",
    "\t# This function was taken from 'http://eayd.in/?p=232' and modified, I found, with the modifications, it to be more\n",
    "\t# effective for known modern words than the function I originally created. But this function worked less well against\n",
    "\t# abbreviations, old english, or slang.\n",
    "\n",
    "\n",
    "def sylco(new_words, verbose=False) :\n",
    "    syllables = []\n",
    "    for word in new_words:\n",
    "        word = str(word.lower())\n",
    "        # exception_add are words that need extra syllables\n",
    "        # exception_del are words that need less syllables\n",
    "\n",
    "        exception_add = ['serious','crucial']\n",
    "        exception_del = ['fortunately','unfortunately']\n",
    "\n",
    "        co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
    "        co_two = ['coapt','coed','coinci']\n",
    "\n",
    "        pre_one = ['preach']\n",
    "\n",
    "        syls = 0 #added syllable number\n",
    "        disc = 0 #discarded syllable number\n",
    "\n",
    "        #1) if letters < 3 : return 1\n",
    "        if len(word) <= 3 :\n",
    "            word_syl = 1\n",
    "            syllables.append(word_syl)\n",
    "            continue\n",
    "\n",
    "        #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
    "        # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.)\n",
    "\n",
    "        if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
    "            doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "            if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
    "                if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
    "                    pass\n",
    "                else :\n",
    "                    disc+=1\n",
    "\n",
    "        #3) discard trailing \"e\", except where ending is \"le\"\n",
    "\n",
    "        le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while']\n",
    "\n",
    "        if word[-1:] == \"e\" :\n",
    "            if word[-2:] == \"le\" and word not in le_except :\n",
    "                pass\n",
    "\n",
    "            else :\n",
    "                disc+=1\n",
    "\n",
    "        #4) check if consecutive vowels exists, triplets or pairs, count them as one.\n",
    "\n",
    "        doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "        tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
    "        disc+=doubleAndtripple + tripple\n",
    "\n",
    "        #5) count remaining vowels in word.\n",
    "        numVowels = len(re.findall(r'[eaoui]',word))\n",
    "\n",
    "        #6) add one if starts with \"mc\"\n",
    "        if word[:2] == \"mc\" :\n",
    "            syls+=1\n",
    "\n",
    "        #7) add one if ends with \"y\" but is not surrouned by vowel\n",
    "        if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
    "            syls +=1\n",
    "\n",
    "        #8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
    "\n",
    "        for i,j in enumerate(word) :\n",
    "            if j == \"y\" :\n",
    "                if (i != 0) and (i != len(word)-1) :\n",
    "                    if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
    "                        syls+=1\n",
    "\n",
    "        #9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one.\n",
    "\n",
    "        if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
    "            syls+=1\n",
    "\n",
    "        if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
    "            syls+=1\n",
    "\n",
    "        #10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
    "\n",
    "        if word[-3:] == \"ian\" :\n",
    "        #and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
    "            if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
    "                pass\n",
    "            else :\n",
    "                syls+=1\n",
    "\n",
    "        #11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "        if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
    "\n",
    "            if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
    "                syls+=1\n",
    "            elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
    "                pass\n",
    "            else :\n",
    "                syls+=1\n",
    "\n",
    "        #12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "        if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
    "            if word[:6] in pre_one :\n",
    "                pass\n",
    "            else :\n",
    "                syls+=1\n",
    "\n",
    "        #13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
    "\n",
    "        negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"]\n",
    "\n",
    "        if word[-3:] == \"n't\" :\n",
    "            if word in negative :\n",
    "                syls+=1\n",
    "            else :\n",
    "                pass\n",
    "\n",
    "        #14) Handling the exceptional words.\n",
    "\n",
    "        if word in exception_del :\n",
    "            disc+=1\n",
    "\n",
    "        if word in exception_add :\n",
    "            syls+=1\n",
    "\n",
    "        # calculate the output\n",
    "        word_syl = numVowels - disc + syls\n",
    "        syllables.append(word_syl)\n",
    "    syllables = [x for x in syllables if x != 0]\n",
    "    return syllables\n",
    "\n",
    "def compile_meter_list(new_words, verbose=True):\n",
    "    # simplifies and compiles cmu cormpus info into listed list\n",
    "    iambic = cmudict.dict()                     # connect to cmu corpus, called iambic\n",
    "    big_list = []                               # list to collect all the different versions of words and their meter\n",
    "    for word in new_words:\n",
    "        word = str(word)                     # get word from list of clean words\n",
    "        syl_num = sylco([word])\n",
    "        word_n_versions_list = []               # list has each word and the different versions\n",
    "        word_n_versions_list.append(word)       # add word\n",
    "        versions_list = []                      # list of all diff versions\n",
    "        try:                                    # if word is in corpus\n",
    "            for n,x in enumerate(iambic[word.lower()]): # get versions for each word\n",
    "                version = []                    # list for each version\n",
    "                version.append(word+str(n))     # add word+version\n",
    "                meter_list = []                 # list holds word version's meter\n",
    "                for y in x:                     # for word in cmu-dict sent\n",
    "                    for char in y:              # for character in word\n",
    "                        if char.isdigit() == True: # if the char is a number\n",
    "                            meter_list.append(int(char)) # add number to meter\n",
    "                version.append(meter_list)      # add meter to the word version\n",
    "                versions_list.append(version)   # add all the versions to one list\n",
    "            word_n_versions_list.append(versions_list) # add list of diff versions to word and versions list\n",
    "            big_list.append(word_n_versions_list)\n",
    "        except:                                 # if word isnt in corpus\n",
    "            version = []                        # empty version\n",
    "            version.append(word+str(0))         # add word1\n",
    "            meter_list = []                     # empty meter list\n",
    "            if len(syl_num) == 1:\n",
    "                for syl in range(syl_num[0]):          # for each syllable...\n",
    "                    meter_list.append(-1)           # add 0 to meter_list\n",
    "                version.append(meter_list)          # add empty meter list to version\n",
    "                versions_list.append(version)       # add version w/ word1 to versions list\n",
    "                word_n_versions_list.append(versions_list) # add list of diff versions to word and versions list\n",
    "                big_list.append(word_n_versions_list) # adds word and versions to big list\n",
    "    return big_list\n",
    "\n",
    "def find_best(line, intended_syllables, optimal=[0,1,0,1,0,1,0,1,0,1,0,1], verbose=True):\n",
    "    # finds best version of meter for each word and creates the best meter for the line.\n",
    "    optimal_line = []\n",
    "    optimal_meter = []\n",
    "\n",
    "    syllable_index = 0\n",
    "\n",
    "    for syllables, word_list in zip(intended_syllables, line):\n",
    "\n",
    "        if verbose:\n",
    "            print('Current syllable index:', syllable_index)\n",
    "            print('Current word key:', word_list)\n",
    "\n",
    "        best_score = float('Inf')\n",
    "        best_inflections = []\n",
    "\n",
    "        for word, inflection_option in word_list[1]:\n",
    "\n",
    "            if not len(inflection_option) == 0:\n",
    "\n",
    "                if len(inflection_option) > syllables:\n",
    "                    inflection_option = inflection_option[:syllables]\n",
    "\n",
    "                current_optimal = optimal[syllable_index:syllable_index+syllables]\n",
    "                score = sum([abs(i - o) for i, o in zip(inflection_option, current_optimal)])\n",
    "\n",
    "                if verbose:\n",
    "                    print('Word inflections option:', inflection_option, 'score:', score)\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_inflections = inflection_option\n",
    "                    best_score = score\n",
    "                elif score == 10 and syllables == 10:\n",
    "                    best_inflections = inflection_option\n",
    "                    best_score = score\n",
    "\n",
    "        optimal_line.append([word_list[0],[word_list[0], best_inflections]])\n",
    "        optimal_meter.append(best_inflections)\n",
    "        syllable_index = syllable_index + syllables\n",
    "\n",
    "        if verbose:\n",
    "            print('Current optimal line:\\n',)\n",
    "            print('-----------------------------------------\\n')\n",
    "\n",
    "    return optimal_line, optimal_meter\n",
    "\n",
    "def get_sentiment(new_words, verbose=False):\n",
    "    line = ' '.join(new_words)\n",
    "    feels = TextBlob(str(line))\n",
    "    return feels.sentiment\n",
    "\n",
    "def parse_sonnet_lines(sonnet_lines, author, verbose=1):\n",
    "\n",
    "    # We're making the DataFrame at the end, from the full set of parsed\n",
    "    # lines, so first set up the \"container\" of columns.\n",
    "    # DataFrames can take numpy arrays where a list of lists is equivalent\n",
    "    # to each row an internal list, columns lengths of the interal lists.\n",
    "    syllable_inflection_columns = []\n",
    "    word_list_column = []\n",
    "    sonnet_num_list = []\n",
    "    author_list = []\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "\n",
    "    # Track how many completed and skipped lines. Ideally the only skipped\n",
    "    # lines are at the beginning and end, but skips regardless.\n",
    "    completed_lines = 0\n",
    "    skipped_lines = 0\n",
    "\n",
    "    # Iterate through the sonnet lines. Keep track of line index\n",
    "    # in case you want to add more information from the sonnet lines later.\n",
    "    for line_index, line in enumerate(sonnet_lines):\n",
    "\n",
    "        # Print out tracking information. It's slow so nice to see where it is.\n",
    "        if verbose == 1:\n",
    "            if (line_index % 10 == 0):\n",
    "                print('sonnet line:', line_index+1, 'complete inflections:', len(syllable_inflection_columns))\n",
    "\n",
    "        # skip if nothing there\n",
    "        if line == np.nan:\n",
    "            line = 'empty'\n",
    "\n",
    "        # Split words in the line. Then cleans the words\n",
    "        words = line.split()\n",
    "        cleaned_words = clean_words(words, verbose=False)\n",
    "\n",
    "        # If len < 2 skip.\n",
    "        if len(cleaned_words) < 1:\n",
    "            continue\n",
    "        elif clean_words == ['empty']:\n",
    "            continue\n",
    "        elif len(cleaned_words) == 1:\n",
    "            sonnet_num = cleaned_words[0]\n",
    "        else:\n",
    "\n",
    "            # Set up the current row for the DataFrame, which is an internal list\n",
    "            # for the list of lists.\n",
    "            syllable_inflection_row = []\n",
    "\n",
    "            # Append the index of the line in the original sonnet_lines list.\n",
    "            # This will be column 1 of the DataFrame.\n",
    "            syllable_inflection_row.append(line_index)\n",
    "\n",
    "            if (verbose == 2):\n",
    "                print('counting syllables')\n",
    "            line_syllables = sylco(cleaned_words, verbose=False)\n",
    "            sum_line_syllables = sum(line_syllables)\n",
    "\n",
    "            # Append the syllable count. Column 2 of the DataFrame.\n",
    "            syllable_inflection_row.append(sum_line_syllables)\n",
    "\n",
    "            # Append sentiment\n",
    "            feels = get_sentiment(cleaned_words)\n",
    "\n",
    "            if (verbose == 2):\n",
    "                print('compiling meter list')\n",
    "            word_listed_list = compile_meter_list(cleaned_words, verbose=False)\n",
    "\n",
    "            if (verbose == 2):\n",
    "                print('finding optimal meter')\n",
    "            optimal_line, optimal_meter = find_best(word_listed_list, line_syllables,\n",
    "                                                    verbose=False)\n",
    "            optimal_meter_compress = [i for sublist in optimal_meter for i in sublist]\n",
    "\n",
    "            # Some lines don't have 12 syllables. For the missing syllables add -1s\n",
    "            # to the end of the list until 12 syllables long.\n",
    "            if len(optimal_meter_compress) < 12:\n",
    "                missing_inflections = [-1 for i in range(12-len(optimal_meter_compress))]\n",
    "                optimal_meter_compress = optimal_meter_compress + missing_inflections\n",
    "\n",
    "            for inflection in optimal_meter_compress:\n",
    "                # Append string indicators for inflections, for dummy coding in your\n",
    "                # model later.\n",
    "                if inflection == 1:\n",
    "                    syllable_inflection_row.append('stress')\n",
    "                elif inflection == 0:\n",
    "                    syllable_inflection_row.append('unstress')\n",
    "                elif inflection == -1:\n",
    "                    syllable_inflection_row.append('missing')\n",
    "\n",
    "            # Check to make sure the row is actually 14 columns long,\n",
    "            # or the DataFrame creation at the end will break.\n",
    "            if len(syllable_inflection_row) == 13:\n",
    "                if verbose:\n",
    "                    print('Missing columns in row!!', len(syllable_inflection_row))\n",
    "                syllable_inflection_row.append('missing')\n",
    "                print(\"Fixed it!!\", len(syllable_inflection_row))\n",
    "                # adds the syllable row\n",
    "                syllable_inflection_columns.append(syllable_inflection_row)\n",
    "                # adds list of words for that row\n",
    "                word_list_column.append(cleaned_words)\n",
    "                # adds which sonnet it is\n",
    "                if author == \"shakespeare\":\n",
    "                    sonnet_num_list.append(int(1+ceil(line_index/16)))\n",
    "                else:\n",
    "                    sonnet_num_list.append(int(1+ceil(line_index/14)))\n",
    "                # adds author\n",
    "                author_list.append(author)\n",
    "                # adds sentiment\n",
    "                polarity_list.append(feels.polarity)\n",
    "                subjectivity_list.append(feels.subjectivity)\n",
    "            elif len(syllable_inflection_row) < 14:\n",
    "                if verbose:\n",
    "                    print('Missing columns in row !!', len(syllable_inflection_row))\n",
    "                    print('LEAVING THIS LINE OUT!!')\n",
    "                    skipped_lines += 1\n",
    "            elif len(syllable_inflection_row) == 15:\n",
    "                if syllable_inflection_row[-1] == \"missing\":\n",
    "                    del syllable_inflection_row[-1]\n",
    "                    print(\"Fixed it !!\", len(syllable_inflection_row))\n",
    "                    # adds the syllable row\n",
    "                    syllable_inflection_columns.append(syllable_inflection_row)\n",
    "                    # adds list of words for that row\n",
    "                    word_list_column.append(cleaned_words)\n",
    "                    # adds which sonnet it is\n",
    "                    if author == \"shakespeare\":\n",
    "                        sonnet_num_list.append(int(1+ceil(line_index/16)))\n",
    "                    else:\n",
    "                        sonnet_num_list.append(int(1+ceil(line_index/14)))\n",
    "                    # adds author\n",
    "                    author_list.append(author)\n",
    "                    # adds sentiment\n",
    "                    polarity_list.append(feels.polarity)\n",
    "                    subjectivity_list.append(feels.subjectivity)\n",
    "            elif len(syllable_inflection_row) > 14:\n",
    "                if verbose:\n",
    "                    print('Too many columns in row !!', len(syllable_inflection_row))\n",
    "                    print('LEAVING THIS LINE OUT!!')\n",
    "                    skipped_lines += 1\n",
    "            else:\n",
    "                # adds the syllable row\n",
    "                syllable_inflection_columns.append(syllable_inflection_row)\n",
    "                # adds list of words for that row\n",
    "                word_list_column.append(cleaned_words)\n",
    "                # adds which sonnet it is\n",
    "                if author == \"shakespeare\":\n",
    "                    sonnet_num_list.append(int(1+ceil(line_index/16)))\n",
    "                else:\n",
    "                    sonnet_num_list.append(int(1+ceil(line_index/14)))\n",
    "                # adds author\n",
    "                author_list.append(author)\n",
    "                # adds sentiment\n",
    "                polarity_list.append(feels.polarity)\n",
    "                subjectivity_list.append(feels.subjectivity)\n",
    "\n",
    "            print('sonnet number', line_index)\n",
    "            completed_lines += 1\n",
    "\n",
    "    if verbose == 1:\n",
    "        print('completed_lines:', completed_lines, 'skipped lines:', skipped_lines)\n",
    "\n",
    "    # Turn the list of lists into a numpy array. This creates a matrix\n",
    "    # of dimensions (num_sonnet_lines x 14).\n",
    "    syllable_inflection_columns = np.array(syllable_inflection_columns)\n",
    "\n",
    "    print(\"FINISHED !!!\")\n",
    "    return syllable_inflection_columns, word_list_column, sonnet_num_list, author_list, polarity_list, subjectivity_list\n",
    "\n",
    "def text_line_parser(noise_list):\n",
    "    text_list = []\n",
    "    meter_list = []\n",
    "\n",
    "    for sind, sentence in enumerate(noise_list):\n",
    "        sentence = str(sentence).split()\n",
    "        cleaned_words = clean_words(sentence)\n",
    "\n",
    "        # print(cleaned_words)\n",
    "        line_syllables = sylco(cleaned_words)\n",
    "\n",
    "        line_cutoff = 0\n",
    "        cutoff_sentence = []\n",
    "        for ind, ls in enumerate(line_syllables):\n",
    "            line_cutoff += ls\n",
    "            if (line_cutoff >= 9) and (line_cutoff <= 12):\n",
    "                cutoff_sentence = cleaned_words[0:ind]\n",
    "            elif line_cutoff > 12:\n",
    "                break\n",
    "\n",
    "        if len(cutoff_sentence) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            #valid_sentences.append(cutoff_sentence)\n",
    "            print('viable sentence index:', sind)\n",
    "\n",
    "            #print line_syllables\n",
    "            word_listed_list = compile_meter_list(cutoff_sentence)\n",
    "            # print(word_listed_list)\n",
    "\n",
    "            error = 3\n",
    "            bad_optimal = [[1,1,1,1,1,1,1,1,1,1,1,1],[0,0,0,0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "            for optimal in bad_optimal:\n",
    "                optimal_line, optimal_meter = find_best(word_listed_list, line_syllables, optimal=optimal, verbose=False)\n",
    "                omc = [item for sublist in optimal_meter for item in sublist]\n",
    "\n",
    "                matched_length = len([x for x, o, in zip(omc, optimal[0:len(omc)]) if x in [o, 2]])\n",
    "\n",
    "                if not (matched_length >= len(omc)-error):\n",
    "                        continue\n",
    "                else:\n",
    "                    # print('actually optimal:', sind)\n",
    "                    text_list.append(cutoff_sentence)\n",
    "                    meter_list.append(omc)\n",
    "    # print(len(text_list), len(meter_list))\n",
    "    return text_list, meter_list\n",
    "\n",
    "# saving:\n",
    "# filepath = open('filename.p', 'w')\n",
    "# cPickle.dump(current_text_lines, filepath)\n",
    "# filepath.close()\n",
    "#\n",
    "# loading:\n",
    "# filepath = open('filename.p', 'r')\n",
    "# loaded_text_lines = cPickle.load(filepath)\n",
    "# filepath.close()\n",
    "\n",
    "\n",
    "# running a script:\n",
    "# first, add the function to the class\n",
    "# change the functions it needs to self.whatever()\n",
    "# at the bottom of the script your class is in:\n",
    "#\n",
    "\n",
    "# this goes at the very bottom at the script:\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     austen_text_file = 'some_file_path'\n",
    "#     pp = PoetryParser()\n",
    "\n",
    "#     austen_text = pp.parse_austen_text()\n",
    "\n",
    "#     pickle_filepath = 'pickle_filepath.p'\n",
    "\n",
    "#     # your new function which is part of PoetryParser now:\n",
    "#     pp.get_invalid_lines(austen_text_file, pickle_filepath)\n",
    "\n",
    "# now, to run the script:\n",
    "# > python poetry_parser.py\n",
    "\n",
    "def text_to_df(text_list, meter_list, author):\n",
    "    syllable_inflection_columns = []\n",
    "    word_list_column = []\n",
    "    sonnet_num_list = []\n",
    "    author_list = []\n",
    "    polarity_list = []\n",
    "    subjectivity_list = []\n",
    "\n",
    "    # Track how many completed and skipped lines. Ideally the only skipped\n",
    "    # lines are at the beginning and end, but skips regardless.\n",
    "    completed_lines = 0\n",
    "    skipped_lines = 0\n",
    "\n",
    "    for line_index, (t, m) in enumerate(zip(text_list, meter_list)):\n",
    "        # print(t, m)\n",
    "        # Set up the current row for the DataFrame, which is an internal list\n",
    "        # for the list of lists.\n",
    "        syllable_inflection_row = []\n",
    "\n",
    "        # Append the index of the line in the original sonnet_lines list.\n",
    "        # This will be column 1 of the DataFrame.\n",
    "        syllable_inflection_row.append(line_index)\n",
    "\n",
    "        print('counting syllables')\n",
    "        line_syllables = sylco(t, verbose=False)\n",
    "        sum_line_syllables = sum(line_syllables)\n",
    "\n",
    "        # Append the syllable count. Column 2 of the DataFrame.\n",
    "        syllable_inflection_row.append(sum_line_syllables)\n",
    "\n",
    "        # Append sentiment\n",
    "        feels = get_sentiment(t)\n",
    "\n",
    "        if len(meter_list) < 12:\n",
    "            missing_inflections = [-1 for i in range(12-len(m))]\n",
    "            optimal_meter_compress = m + missing_inflections\n",
    "\n",
    "        for inflection in m:\n",
    "            # Append string indicators for inflections, for dummy coding in your\n",
    "            # model later.\n",
    "            if inflection == 1:\n",
    "                syllable_inflection_row.append('stress')\n",
    "            elif inflection == 0:\n",
    "                syllable_inflection_row.append('unstress')\n",
    "            elif inflection == -1:\n",
    "                syllable_inflection_row.append('missing')\n",
    "            elif inflection == 2:\n",
    "                if syllable_inflection_row[-1] == 1:\n",
    "                    syllable_inflection_row.append('unstress')\n",
    "                elif syllable_inflection_row[-1] == 0:\n",
    "                    syllable_inflection_row.append('stress')\n",
    "                else:\n",
    "                    syllable_inflection_row.append('unstress')\n",
    "\n",
    "                # Check to make sure the row is actually 14 columns long,\n",
    "                # or the DataFrame creation at the end will break.\n",
    "        if len(syllable_inflection_row) == 13:\n",
    "            print('Missing columns in row!!', len(syllable_inflection_row))\n",
    "            syllable_inflection_row.append('missing')\n",
    "            print(\"Fixed it!!\", len(syllable_inflection_row))\n",
    "            # adds the syllable row\n",
    "            syllable_inflection_columns.append(syllable_inflection_row)\n",
    "            # adds list of words for that row\n",
    "            word_list_column.append(t)\n",
    "            # adds which sonnet it is\n",
    "            sonnet_num_list.append(line_index)\n",
    "            # adds author\n",
    "            author_list.append(author)\n",
    "            # adds sentiment\n",
    "            polarity_list.append(feels.polarity)\n",
    "            subjectivity_list.append(feels.subjectivity)\n",
    "        elif len(syllable_inflection_row) < 14:\n",
    "            print('Missing columns in row !!', len(syllable_inflection_row))\n",
    "            print('LEAVING THIS LINE OUT!!')\n",
    "            skipped_lines += 1\n",
    "        elif len(syllable_inflection_row) == 15:\n",
    "            if syllable_inflection_row[-1] == \"missing\":\n",
    "                del syllable_inflection_row[-1]\n",
    "                print(\"Fixed it !!\", len(syllable_inflection_row))\n",
    "                # adds the syllable row\n",
    "                syllable_inflection_columns.append(syllable_inflection_row)\n",
    "                # adds list of words for that row\n",
    "                word_list_column.append(t)\n",
    "                # adds which sonnet it is\n",
    "                sonnet_num_list.append(line_index)\n",
    "                # adds author\n",
    "                author_list.append(author)\n",
    "                # adds sentiment\n",
    "                polarity_list.append(feels.polarity)\n",
    "                subjectivity_list.append(feels.subjectivity)\n",
    "        elif len(syllable_inflection_row) > 14:\n",
    "            print('Too many columns in row !!', len(syllable_inflection_row))\n",
    "            print('LEAVING THIS LINE OUT!!')\n",
    "            skipped_lines += 1\n",
    "        else:\n",
    "            # adds the syllable row\n",
    "            syllable_inflection_columns.append(syllable_inflection_row)\n",
    "            # adds list of words for that row\n",
    "            word_list_column.append(t)\n",
    "            # adds which sonnet it is\n",
    "            sonnet_num_list.append(line_index)\n",
    "            # adds author\n",
    "            author_list.append(author)\n",
    "            # adds sentiment\n",
    "            polarity_list.append(feels.polarity)\n",
    "            subjectivity_list.append(feels.subjectivity)\n",
    "\n",
    "        print('sonnet number', line_index)\n",
    "        completed_lines += 1\n",
    "\n",
    "    print('completed_lines:', completed_lines, 'skipped lines:', skipped_lines)\n",
    "\n",
    "    # Turn the list of lists into a numpy array. This creates a matrix\n",
    "    # of dimensions (num_sonnet_lines x 14).\n",
    "    syllable_inflection_columns = np.array(syllable_inflection_columns)\n",
    "\n",
    "    print(\"FINISHED !!!\")\n",
    "    return syllable_inflection_columns, word_list_column, sonnet_num_list, author_list, polarity_list, subjectivity_list\n",
    "\n",
    "def create_dataframe(syllable_inflection_columns, word_list_column, sonnet_num_list, author_list, polarity_list, subjectivity_list):\n",
    "    # Set up column names.\n",
    "    column_names = ['sonnet_index','syllables','s1','s2','s3','s4','s5',\n",
    "                    's6','s7','s8','s9','s10','s11','s12']\n",
    "\n",
    "    # Turn the matrix into a DataFrame with the column names.\n",
    "    sonnet_df = pd.DataFrame(syllable_inflection_columns, columns=column_names)\n",
    "    sonnet_df['word_list'] = word_list_column\n",
    "    sonnet_df['sonnet_num'] = sonnet_num_list\n",
    "    sonnet_df['author'] = author_list\n",
    "    sonnet_df['polarity'] = polarity_list\n",
    "    sonnet_df['subjectivity'] = subjectivity_list\n",
    "    return sonnet_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
